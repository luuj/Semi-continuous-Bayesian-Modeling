pi1 <- function(beta1, X, y1, beta1y, alpha1, c=0, V1=0){
inv.logit(alpha1 + X%*%beta1 + y1*beta1y + c + V1)
}
pi2 <- function(beta2, X, y1, beta2y, alpha2, V2=0){
inv.logit(alpha2 + X%*%beta2 + y1*beta2y + V2)
}
theta <- function(betat, X, alphat, Vtheta=0){
inv.logit(alphat + X%*%betat + Vtheta)
}
pi12 <- function(theta, pi1, pi2){
if (theta > 0.99 & theta < 1.01){
return(pi1*pi2)
}
a = (pi1 + pi2) * (theta-1)
(1/(2*(theta-1))) * (1 + a - sqrt( (1 + a)^2 - 4*theta*(theta-1)*pi1*pi2))
}
# Function to calculate the continuous portion of the model
mu <- function(betam, X, alpham, d=0, Vmu=0){
exp(alpham + X%*%betam + d + Vmu)
}
# Calculates the gamma distribution likelihood for the continuous portion
gammaLik <- function(par, x){
alpha <- par[1]
beta <- par[2]
n <- length(x)
out <- (alpha-1)*sum(log(x))-(1/beta)*sum(x) - n*alpha*log(beta) - n*log(gamma(alpha))
return(out)
}
# Converts shape and rate terms for each trt group into alpham and betam
getGammaPar <- function(shape, rate1, rate2){
alpham <- log(rate1*shape)
betam <- log(rate2*shape) - alpham
return(c(alpham=alpham,betam=betam))
}
# Converts alpham and betam into rate1 and rate2
convertGammaPar <- function(shape, alpham, betam){
rate1 <- exp(alpham) / shape
rate2 <- exp(betam + alpham) / shape
return(c(rate1=rate1,rate2=rate2))
}
### Generate data
# n_k = Number of individuals per cluster
# n_t = Number of observations per individual
# k = Number of clusters
gen_data <- function(n_k=500, n_t=10, k=4,
bSigma=matrix(c(3,0,0,3), nrow=2),
vSigma=(diag(4)*2),
coefs=c(1.3,-0.5,1,0.8,-1.3,3,-1,-0.5,0.5,1)){
# Create base data frame
n <- n_k*k
id <- factor(rep(1:n, each=n_t))
t <- factor(rep(70:(70+n_t-1),n))
trt <- rep(rbinom(n,1,0.5),each=n_t)
clst <- factor(rep(1:k, each=(n_k*n_t)))
dat <- tibble(id,t,trt,clst,y1=NA,y2=NA,y1_prev=0,cost=0,cumcost=0)
# Known coefficients
beta <- tibble(beta1=coefs[1], beta2=coefs[2], betat=coefs[3], beta1y=coefs[4], beta2y=coefs[5], betam=coefs[6])
alpha <- tibble(alpha1=coefs[7], alpha2=coefs[8], alphat=coefs[9], alpham=coefs[10])
# Generate cluster-level random effects
V <- mvrnorm(n=k, mu=rep(0,4), Sigma=vSigma)
colnames(V) <- c("V1", "V2", "Vmu", "Vtheta")
dat <- cbind(dat,V[dat$clst,])
genIndividual <- function(dat.in){
base <- dat.in[1,]
# Generate subject-level random effects
b <- t(mvrnorm(n=1, mu=c(0,0), Sigma=bSigma))
colnames(b) <- c("b1", "bmu")
dat.in <- cbind(dat.in,b)
# Calculate values of pi1 and pi2, given previous y1
theta <- theta(beta$betat, base$trt, alpha$alphat, base$Vtheta)
pi1_0 <- pi1(beta$beta1, base$trt, 0, beta$beta1y, alpha$alpha1, b[1], base$V1)
pi1_1 <- pi1(beta$beta1, base$trt, 1, beta$beta1y, alpha$alpha1, b[1], base$V1)
pi2_0 <- pi2(beta$beta2, base$trt, 0, beta$beta2y, alpha$alpha2, base$V2)
pi2_1 <- pi2(beta$beta2, base$trt, 1, beta$beta2y, alpha$alpha2, base$V2)
pi12_0 <- pi12(theta,pi1_0,pi2_0)
pi12_1 <- pi12(theta,pi1_1,pi2_1)
# Calculate multinomial probabilities
prob_0000 <- 1-pi1_0-pi2_0+pi12_0
prob_0001 <- pi2_0-pi12_0
prob_0010 <- pi1_0-pi12_0
prob_0011 <- pi12_0
prob_1000 <- 1-pi1_1-pi2_1+pi12_1
prob_1001 <- pi2_1-pi12_1
prob_1010 <- pi1_1-pi12_1
prob_1011 <- pi12_1
base_cost <- mu(beta$betam, base$trt, alpha$alpham, b[2], base$Vmu)
shape <- 5
for(i in 1:n_t){
# Store previous non-terminal event
if(i > 1)
dat.in[i,]$y1_prev <- dat.in[i-1,]$y1
# Calculate cost accrued based on gamma distribution
cost <- rgamma(1,shape=shape,scale=base_cost/shape)
# Choose outcome
if (dat.in[i,]$y1_prev == 0){
out <- which(rmultinom(1,1,c(prob_0000, prob_0001, prob_0010, prob_0011))==1)
switch(out,
{dat.in[i,c("y1","y2","cost")] <- list(0,0,0)},
{dat.in[i,c("y1","y2","cost")] <- list(0,1,0)},
{dat.in[i,c("y1","y2","cost")] <- list(1,0,cost)},
{dat.in[i,c("y1","y2","cost")] <- list(1,1,cost)})
}
else{
out <- which(rmultinom(1,1,c(prob_1000, prob_1001, prob_1010,prob_1011))==1)
switch(out,
{dat.in[i,c("y1","y2","cost")] <- list(0,0,0)},
{dat.in[i,c("y1","y2","cost")] <- list(0,1,0)},
{dat.in[i,c("y1","y2","cost")] <- list(1,0,cost)},
{dat.in[i,c("y1","y2","cost")] <- list(1,1,cost)})
}
# Stop if terminal event
if (dat.in[i,]$y2 == 1){
break
}
}
# Calculate cumulative cost
dat.in$cumcost <- cumsum(dat.in$cost)
return(dat.in %>% na.omit())
}
dat %>% group_by(id) %>% group_modify(~genIndividual(.)) %>% ungroup()
}
# Explore clustered data generation
set.seed(3)
dat <- gen_data()
gt(dat$clst)
gt(dat)
library(gtools)
library(dplyr)
library(lme4)
library(MASS)
library(ggplot2)
# Likelihood calculator
# Wrapper for binary likelihood
lb <- function(par, dat){
beta <- data.frame(beta1=par[1], beta2=par[2], betat=par[3], beta1y=par[4], beta2y=par[5])
alpha <- data.frame(alpha1=par[6], alpha2=par[7], alphat=par[8])
loglik_bin(dat,beta,alpha)
}
# Wrapper for continuous likelihood
lc <- function(par, dat){
gamma <- data.frame(shape=par[9], rate1=par[10], rate2=par[11])
loglik_con(dat,gamma)
}
# Calculate full likelihood for binary portion
loglik_bin <- function(dat, beta, alpha){
# Obtain probabilities for each covariate
trt <- unique(dat$trt)
trt_n <- length(trt)
ti <- dat$trt+1
coefs <- data.frame(matrix(nrow = trt_n, ncol=6))
colnames(coefs) <- c("pi1_0", "pi1_1", "pi2_0", "pi2_1", "pi12_0", "pi12_1")
for (i in 1:trt_n){
# Calculate values of pi1 and pi2, given previous y1
x <- trt[i]
theta <- theta(beta$betat, x, alpha$alphat)
coefs$pi1_0[i] <- pi1(beta$beta1, x, 0, beta$beta1y, alpha$alpha1)
coefs$pi1_1[i] <- pi1(beta$beta1, x, 1, beta$beta1y, alpha$alpha1)
coefs$pi2_0[i] <- pi2(beta$beta2, x, 0, beta$beta2y, alpha$alpha2)
coefs$pi2_1[i] <- pi2(beta$beta2, x, 1, beta$beta2y, alpha$alpha2)
coefs$pi12_0[i] <- pi12(theta, coefs$pi1_0[i], coefs$pi2_0[i])
coefs$pi12_1[i] <- pi12(theta,coefs$pi1_1[i], coefs$pi2_1[i])
}
# Probability (prob_abcd) of cd occurring, given that ab is the current state
prob_0000 <- ((1-dat$y1) * (1-dat$y2))*log(1-coefs$pi1_0[ti]-coefs$pi2_0[ti]+coefs$pi12_0[ti])
prob_0001 <- ((1-dat$y1) * dat$y2)*log(coefs$pi2_0[ti]-coefs$pi12_0[ti])
prob_0010 <- (dat$y1 * (1-dat$y2))*log(coefs$pi1_0[ti]-coefs$pi12_0[ti])
prob_0011 <- dat$y1*dat$y2*log(coefs$pi12_0[ti])
prob_1000 <- ((1-dat$y1) * (1-dat$y2))*log(1-coefs$pi1_1[ti]-coefs$pi2_1[ti]+coefs$pi12_1[ti])
prob_1001 <- ((1-dat$y1) * dat$y2)*log(coefs$pi2_1[ti]-coefs$pi12_1[ti])
prob_1010 <- (dat$y1 * (1-dat$y2))*log(coefs$pi1_1[ti]-coefs$pi12_1[ti])
prob_1011 <- dat$y1*dat$y2*log(coefs$pi12_1[ti])
# Likelihood for binary portion
sum((1-dat$y1_prev)*(prob_0011+prob_0010+prob_0001+prob_0000) + (dat$y1_prev)*(prob_1000+prob_1001+prob_1011+prob_1010))
}
# Calculate full likelihood for continuous portion
loglik_con <- function(dat, gamma){
conLik <- dat %>% filter(cost > 0 & trt==0) %>% pull(cost) %>% gammaLik(c(gamma$shape,gamma$rate1), .)
conLik <- conLik + dat %>% filter(cost > 0 & trt==1) %>% pull(cost) %>% gammaLik(c(gamma$shape,gamma$rate2), .)
return(conLik)
}
# Get probability quantities pi1, pi2, pi12
getPi <- function(dat, beta, alpha){
base <- dat[1,]
n_row <- nrow(dat)
# Calculate values of pi1 and pi2, given previous y1
theta <- theta(beta$betat, base$trt, alpha$alphat)
pi1_0 <- pi1(beta$beta1, base$trt, 0, beta$beta1y, alpha$alpha1)
pi1_1 <- pi1(beta$beta1, base$trt, 1, beta$beta1y, alpha$alpha1)
pi2_0 <- pi2(beta$beta2, base$trt, 0, beta$beta2y, alpha$alpha2)
pi2_1 <- pi2(beta$beta2, base$trt, 1, beta$beta2y, alpha$alpha2)
pi12_0 <- pi12(theta,pi1_0,pi2_0)
pi12_1 <- pi12(theta,pi1_1,pi2_1)
return(data.frame(pi1_0=rep(pi1_0,each=n_row),pi1_1=rep(pi1_1,each=n_row),
pi2_0=rep(pi2_0,each=n_row),pi2_1=rep(pi2_1,each=n_row),
pi12_0=rep(pi12_0,each=n_row),pi12_1=rep(pi12_1,each=n_row)))
}
# Helper functions to calculate equations 5-7
pi1 <- function(beta1, X, y1, beta1y, alpha1, c=0, V1=0){
inv.logit(alpha1 + X%*%beta1 + y1*beta1y + c + V1)
}
pi2 <- function(beta2, X, y1, beta2y, alpha2, V2=0){
inv.logit(alpha2 + X%*%beta2 + y1*beta2y + V2)
}
theta <- function(betat, X, alphat, Vtheta=0){
inv.logit(alphat + X%*%betat + Vtheta)
}
pi12 <- function(theta, pi1, pi2){
if (theta > 0.99 & theta < 1.01){
return(pi1*pi2)
}
a = (pi1 + pi2) * (theta-1)
(1/(2*(theta-1))) * (1 + a - sqrt( (1 + a)^2 - 4*theta*(theta-1)*pi1*pi2))
}
# Function to calculate the continuous portion of the model
mu <- function(betam, X, alpham, d=0, Vmu=0){
exp(alpham + X%*%betam + d + Vmu)
}
# Calculates the gamma distribution likelihood for the continuous portion
gammaLik <- function(par, x){
alpha <- par[1]
beta <- par[2]
n <- length(x)
out <- (alpha-1)*sum(log(x))-(1/beta)*sum(x) - n*alpha*log(beta) - n*log(gamma(alpha))
return(out)
}
# Converts shape and rate terms for each trt group into alpham and betam
getGammaPar <- function(shape, rate1, rate2){
alpham <- log(rate1*shape)
betam <- log(rate2*shape) - alpham
return(c(alpham=alpham,betam=betam))
}
# Converts alpham and betam into rate1 and rate2
convertGammaPar <- function(shape, alpham, betam){
rate1 <- exp(alpham) / shape
rate2 <- exp(betam + alpham) / shape
return(c(rate1=rate1,rate2=rate2))
}
### Generate data
# n_k = Number of individuals per cluster
# n_t = Number of observations per individual
# k = Number of clusters
gen_data <- function(n_k=500, n_t=10, k=4,
bSigma=matrix(c(3,0,0,3), nrow=2),
vSigma=(diag(4)*2),
coefs=c(1.3,-0.5,1,0.8,-1.3,3,-1,-0.5,0.5,1)){
# Create base data frame
n <- n_k*k
id <- factor(rep(1:n, each=n_t))
t <- factor(rep(70:(70+n_t-1),n))
trt <- rep(rbinom(n,1,0.5),each=n_t)
clst <- factor(rep(1:k, each=(n_k*n_t)))
dat <- tibble(id,t,trt,clst,y1=NA,y2=NA,y1_prev=0,cost=0,cumcost=0)
# Known coefficients
beta <- tibble(beta1=coefs[1], beta2=coefs[2], betat=coefs[3], beta1y=coefs[4], beta2y=coefs[5], betam=coefs[6])
alpha <- tibble(alpha1=coefs[7], alpha2=coefs[8], alphat=coefs[9], alpham=coefs[10])
# Generate cluster-level random effects
V <- mvrnorm(n=k, mu=rep(0,4), Sigma=vSigma)
colnames(V) <- c("V1", "V2", "Vmu", "Vtheta")
dat <- cbind(dat,V[dat$clst,])
genIndividual <- function(dat.in){
base <- dat.in[1,]
# Generate subject-level random effects
b <- t(mvrnorm(n=1, mu=c(0,0), Sigma=bSigma))
colnames(b) <- c("b1", "bmu")
dat.in <- cbind(dat.in,b)
# Calculate values of pi1 and pi2, given previous y1
theta <- theta(beta$betat, base$trt, alpha$alphat, base$Vtheta)
pi1_0 <- pi1(beta$beta1, base$trt, 0, beta$beta1y, alpha$alpha1, b[1], base$V1)
pi1_1 <- pi1(beta$beta1, base$trt, 1, beta$beta1y, alpha$alpha1, b[1], base$V1)
pi2_0 <- pi2(beta$beta2, base$trt, 0, beta$beta2y, alpha$alpha2, base$V2)
pi2_1 <- pi2(beta$beta2, base$trt, 1, beta$beta2y, alpha$alpha2, base$V2)
pi12_0 <- pi12(theta,pi1_0,pi2_0)
pi12_1 <- pi12(theta,pi1_1,pi2_1)
# Calculate multinomial probabilities
prob_0000 <- 1-pi1_0-pi2_0+pi12_0
prob_0001 <- pi2_0-pi12_0
prob_0010 <- pi1_0-pi12_0
prob_0011 <- pi12_0
prob_1000 <- 1-pi1_1-pi2_1+pi12_1
prob_1001 <- pi2_1-pi12_1
prob_1010 <- pi1_1-pi12_1
prob_1011 <- pi12_1
base_cost <- mu(beta$betam, base$trt, alpha$alpham, b[2], base$Vmu)
shape <- 5
for(i in 1:n_t){
# Store previous non-terminal event
if(i > 1)
dat.in[i,]$y1_prev <- dat.in[i-1,]$y1
# Calculate cost accrued based on gamma distribution
cost <- rgamma(1,shape=shape,scale=base_cost/shape)
# Choose outcome
if (dat.in[i,]$y1_prev == 0){
out <- which(rmultinom(1,1,c(prob_0000, prob_0001, prob_0010, prob_0011))==1)
switch(out,
{dat.in[i,c("y1","y2","cost")] <- list(0,0,0)},
{dat.in[i,c("y1","y2","cost")] <- list(0,1,0)},
{dat.in[i,c("y1","y2","cost")] <- list(1,0,cost)},
{dat.in[i,c("y1","y2","cost")] <- list(1,1,cost)})
}
else{
out <- which(rmultinom(1,1,c(prob_1000, prob_1001, prob_1010,prob_1011))==1)
switch(out,
{dat.in[i,c("y1","y2","cost")] <- list(0,0,0)},
{dat.in[i,c("y1","y2","cost")] <- list(0,1,0)},
{dat.in[i,c("y1","y2","cost")] <- list(1,0,cost)},
{dat.in[i,c("y1","y2","cost")] <- list(1,1,cost)})
}
# Stop if terminal event
if (dat.in[i,]$y2 == 1){
break
}
}
# Calculate cumulative cost
dat.in$cumcost <- cumsum(dat.in$cost)
return(dat.in %>% na.omit())
}
dat %>% group_by(id) %>% group_modify(~genIndividual(.)) %>% ungroup()
}
# Explore clustered data generation
set.seed(3)
dat <- gen_data()
table(dat$clst)
dat %>% dplyr::select(V1:Vtheta) %>% unique()
# V1 and b1 - probability of accruing a cost
# V2 - probability of dying
# Vmu and bmu - how much cost is accrued
# Vtheta - relationship between cost and death
View(dat)
hist(dat$y1)
barplot(dat$y1)
dat$y1
dat[,"y1":"y2"]
dat[,3:4]
dat[,5:6]
stocks <- tibble(
time = as.Date("2009-01-01") + 0:9,
X = rnorm(10, 0, 1),
Y = rnorm(10, 0, 2),
Z = rnorm(10, 0, 4)
)
stocks
gather(dat[,5:6], "Cost", "Death")
library(tidyr)
gather(dat[,5:6], "Cost", "Death")
gather(dat[,5:6], "Type", "Value")
plotProp <- gather(dat[,5:6], "Type", "Value")
plotProp
plotProp %>%
mutate(Type = replace(Type, Type == "y1", "Cost"))
plotProp %>%
mutate(Type = replace(Type, Type == "y1", "Cost"),
Type = replace(Type, Type == "y2", "Death"))
plotProp %<>%
mutate(Type = replace(Type, Type == "y1", "Cost"),
Type = replace(Type, Type == "y2", "Death"))
plotProp <- plotProp %>%
mutate(Type = replace(Type, Type == "y1", "Cost"),
Type = replace(Type, Type == "y2", "Death"))
View(plotProp)
head(plotProp)
plotProp %>% ggplot(aes(x=Type, fill=Value)) + geom_bar(position="fill")
plotProp %>% mutate(Value = factor(Value))
plotProp <- %>% mutate(Value = factor(Value), Type=factor(Type))
plotProp <- plotProp %>%
mutate(Value = factor(Value), Type=factor(Type))
plotProp
plotProp %>% ggplot(aes(x=Type, fill=Value)) + geom_bar(position="fill")
plotProp <- gather(dat[,5:6], "Type", "Value")
plotProp <- plotProp %>%
mutate(Type = replace(Type, Type == "y1", factor("Cost")),
Type = replace(Type, Type == "y2", factor("Death")))
plotProp
plotProp <- plotProp %>%
mutate(Type = replace(Type, Type == "y1", as.factor("Cost")),
Type = replace(Type, Type == "y2", as.factor("Death")))
plotProp
plotProp <- gather(dat[,5:6], "Type", "Value")
plotProp <- plotProp %>%
mutate(Type = replace(Type, Type == "y1", "Cost"),
Type = replace(Type, Type == "y2", "Death"))
plotProp <- plotProp %>%
mutate(Value = factor(Value), Type=factor(Type))
plotProp %>% ggplot(aes(x=Type, fill=Value)) + geom_bar(position="fill")
plotProp %>% ggplot(aes(x=Type, fill=Value)) + geom_bar(position="fill") +
ylab("Proportion") + ggtitle("Proportion of costs accrued and deaths")
dat
summary(dat)
library(shiny); runApp('Library/CloudStorage/OneDrive-HarvardUniversity/GitHub/Semi-continuous-Bayesian-Modeling/Main simulations/shinyapp.R')
runApp('Library/CloudStorage/OneDrive-HarvardUniversity/GitHub/Semi-continuous-Bayesian-Modeling/Main simulations/shinyapp.R')
###################################
########## Question 4 #############
###################################
library(survival)
#### We will use the sample data set
#### set the working directory
surv_data = read.csv("/Users/jonathanluu/Library/CloudStorage/Dropbox/BIO223/BST223_2023/Assignments/Homework_2/Code/Q1data.csv")
# Part a
MIstudy <- read.csv("~/Users/jonathanluu/Library/CloudStorage/Dropbox/BIO223/BST223_2023/Assignments/Homework_2/Code/MIstudy_2023.csv")
# Part a
MIstudy <- read.csv("~/Users/jonathanluu/Library/CloudStorage/Dropbox/BIO223/BST223_2023/Assignments/Homework_2/Code/MIstudy_2023.csv")
# Part a
MIstudy <- read.csv("/Users/jonathanluu/Library/CloudStorage/Dropbox/BIO223/BST223_2023/Assignments/Homework_2/Code/MIstudy_2023.csv")
plot(survfit( Surv(dthtime,dthstat) ~ obese_ovwt,data=MIstudy),
mark.time=TRUE, #this makes the censoring times show up
lty=1:2,main="Survival curves for \n obese/overweight vs normal BMI",
xlab="time (months)",ylab="Survival probability")
legend("topright",legend=c("not obese/overweight","obese/overweight"),lty=1:2)
table(MIstudy$obese_ovwt)/nrow(MIstudy)
plot(survfit( Surv(dthtime,dthstat) ~ obese_ovwt,data=MIstudy),
mark.time=TRUE, #this makes the censoring times show up
lty=1:2,main="Survival curves for \n obese/overweight vs normal BMI",
xlab="time (months)",ylab="Survival probability", conf.int=TRUE)
library(shiny); runApp('Library/CloudStorage/OneDrive-HarvardUniversity/GitHub/Semi-continuous-Bayesian-Modeling/Main simulations/shinyapp.R')
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, comment = NA)
library(tidyverse)
library(readxl)
library(lubridate)
# Read in data
path <- "C:\\Users\\Jonathan\\Dropbox\\Caring Health\\Data\\Caring_Data_V2.xlsx"
#path <- "/Users/jonathanluu/Library/CloudStorage/Dropbox/Caring Health/Data/Caring_Data_V2.xlsx"
dat <- lapply(excel_sheets(path = path), function(x)
read_excel(path = path, sheet = x, col_types = c(rep("guess",20),"text","logical")))
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, comment = NA)
library(tidyverse)
library(readxl)
library(lubridate)
# Read in data
#path <- "C:\\Users\\Jonathan\\Dropbox\\Caring Health\\Data\\Caring_Data_V2.xlsx"
path <- "/Users/jonathanluu/Library/CloudStorage/Dropbox/Caring Health/Data/Caring_Data_V2.xlsx"
dat <- lapply(excel_sheets(path = path), function(x)
read_excel(path = path, sheet = x, col_types = c(rep("guess",20),"text","logical")))
dat <- bind_rows(dat)
# Create categorical visit
dat <- dat %>% mutate(cat_visit = case_when(num_of_visits <= 1 ~ 0,
num_of_visits > 1 & num_of_visits <= 3 ~ 1,
num_of_visits > 3 & num_of_visits <= 8 ~ 2,
num_of_visits > 8 ~ 3))
# Create categorical age
dat <- dat %>% mutate(cat_age = case_when(Age <= 39 ~ 0,
Age > 39 & Age <= 64 ~ 1,
Age > 64 ~ 2))
# Create provider categories
dat <- dat %>% mutate(cat_provider = case_when(TakenProviderType == "Nurse Practitioner" |
TakenProviderType == "Registered Nurse" |
TakenProviderType == "Adult Nurse Practitioner" |
TakenProviderType == "Physician Assistant" |
TakenProviderType == "Physician" ~ "Provider",
TakenProviderType == "Medical Assistant" ~ "MA",
TakenProviderType == "Interpreter Only" ~ "Interpreter",
TakenProviderType == "Community Health Worker" |
TakenProviderType == "Community Support" ~ "CHW",
TakenProviderType == "Behavioral Health Specialist" |
TakenProviderType == "Licensed Independent Clinical Social Worker" |
TakenProviderType == "Licensed Certified Social Worker" |
TakenProviderType == "Licensed Clinical Social Worker" ~ "Behavorial"))
# Create hard-to-pay flag
dat <- dat %>%
mutate(htp = ifelse((FLO_MEAS_ID %in% c(2255,2256,2257,2258,2259,2260,2262,2263,2264,2265)) & (RESPONSE == "Yes"), 1, 0))
# Create help-with flag
dat <- dat %>% mutate(hw = ifelse(dat$FLO_MEAS_ID == 6372, 1, 0))
# Create positive-response flag
dat <- dat %>% mutate(pr = ifelse(FLAG_ABN==1, 1, 0), pr = ifelse(is.na(pr),0,pr))
# Rename some columns
dat <- rename(dat, Language = "LANGUAGE_NAME", Ethnicity = "ETHNIC_GROUP", Race = "RACE")
# Format dates
dat <- dat %>% mutate(ENC_DATE = ymd(ENC_DATE), Last_Visit_Date = ymd(Last_Visit_Date))
# Create encounter year
dat <- dat %>% mutate(enc_year = year(ENC_DATE))
# Get distinct IDs
dat.distinct <- dat %>% distinct(PAT_ID, .keep_all = T)
# Create new dataset showing number of responses by id and encounter
dat.encounter <- dat %>% count(PAT_ID, DOMAIN_GRP, ENC_DATE) %>% mutate_all(na_if,"") %>%
pivot_wider(names_from = DOMAIN_GRP, values_from = n)
dat.encounter <- dat.encounter %>% mutate_if(is.integer, ~replace(.,is.na(.),0))
dat.encounter <- dat.encounter %>% select(-"NA") %>% mutate(sum = rowSums(across(where(is.numeric))))
covar <- dat.distinct %>% select(PAT_ID, cat_age, cat_visit, Race, Ethnicity, Language, HasChargeVisit)
dat.encounter <- merge(dat.encounter,covar,all.x=T)
screen.count <- dat.encounter %>% filter(sum>0) %>% count(PAT_ID)
dat.encounter <- full_join(dat.encounter, screen.count, by="PAT_ID") %>% rename("n_encounter" = "n") # Append number of encounters to dat.encounter
dat <- full_join(dat, screen.count, by="PAT_ID") %>% rename("n_encounter" = "n") # Append number of encounters to dat
# Create dataset of those screened
response_ids <- dat.encounter %>% filter(sum > 0) %>% distinct(PAT_ID) %>% c() %>% .[[1]]
dat.screened <- dat %>% filter(PAT_ID %in% response_ids)
dat.distinct.screened <- dat.screened %>% distinct(PAT_ID, .keep_all = T)
# Flagged ABN
getResponse <- function(questionType){
resp <- dat %>% filter(FLAG_ABN == 1) %>% filter(MEAS_DISP_NAME == questionType) %>% pull(RESPONSE) %>%
table() %>% as_tibble() %>% rename(!!questionType:=".")
}
questions <- dat %>% filter(FLAG_ABN == 1) %>%
select(MEAS_DISP_NAME) %>% unique() %>% unlist() %>% unname() # All response types with flags
lapply(questions,getResponse)
rnorm(0,1)
rnorm(1,0,1)
rnorm(1,0,1)
library(MASS)
library(dplyr)
library(gtools)
setwd("~/Library/CloudStorage/OneDrive-HarvardUniversity/GitHub/Semi-continuous-Bayesian-Modeling/Semi-continuous-Bayesian-Modeling/Main simulations")
source("jags_re.R")
library("dplyr")
desktop <- "/Users/jonathanluu/Desktop/Simulations/"
# Load in all the data files
jagFileCount <- 150
loadRData <- function(fileName,num){
load(fileName, .GlobalEnv)
assign(paste0("jags",i), results, .GlobalEnv)
rm(results, envir=.GlobalEnv)
}
for(i in 1:jagFileCount){
tryCatch({loadRData(paste0(desktop,"Scenario 4 - Death/jags",i,".Rdata"), i)},error=function(cond){})
}

dat.cl$bmi <- (dat.cl$bmi - 30)/5
dat.cl$age <- (dat.cl$age - 45)/5
# GLMM
glmm <- glmer(y ~ trt + age + bmi + (1 | cluster), family="binomial", data=dat.cl)
summary(glmm)
2.18/5
0.34/5
set.seed(4)
dat.cl <- gen_data(param=c(14.24,0.22,-0.07,-0.44), x=design_matrix, n_j=n_j, J=J, sigma_v=0.5)
stored.vk <- unique(dat.cl$V_j)
dat.cl <- dat.cl %>% select(-V_j)
glmm <- glmer(y ~ trt + age + bmi + (1 | cluster), family="binomial", data=dat.cl)
summary(glmm)
# Initialize parameters
n_iter <- 70000
burn_in <- 3000
theta.init <- c(rep(0,ncol(dat.cl)+J-1),0.5)
gamma.init <- c(1,1)
jump_sigma <- c(0.25,0.2,0.01,0.01,rep(0.2, J),0.2)
y <- as.matrix(dat.cl %>% select(y))
x <- as.matrix(dat.cl %>% select(-y))
source("BLR_func.R")
test(10)
source("BLR_func.R")
test(10)
source("BLR_func.R")
test(10)
set.seed(3)
test(10)
set.seed(3)
test(10)
test(10)
# Conditional log likelihood for beta
log_lik.beta <- function(beta, y, x, vk){
p <- expit(x%*%beta + vk) # probability of success
sum(dbinom(y, size=1, prob=p, log=T))
}
# Conditional log likelihood for vk
log_lik.vk <- function(beta, y, x, vk, sigma_v){
log_lik.beta(beta,y,x,vk) + dnorm(vk,0,sigma_v,log = T)
}
# Conditional log likelihood for sigmav
log_lik.sv <- function(sigma_v, vk, num_cluster, a, b){
dgamma(sigma_v, shape=a + num_cluster/2, rate=b + sum(vk^2)/2, log = T)
}
# Metropolis-Hastings algorithm
# y - outcome values
# x - covariate matrix
# theta_init - starting beta, vk, and sigma_v values for MH algorithm
# gamma_init - starting gamma prior distribution parameters
# jump_sigma - variance for proposal distribution
# n_iter - number of MH steps
# burn_in - number of iterations to remove for burn-in
MH.c <- function(y, x, theta_init, gamma_init, jump_sigma, n_iter, burn_in, verbose=F){
# Total # of iterations
num_iter <- n_iter + burn_in + 1
# Record count information
num_total <- length(theta_init)
num_cluster <- length(unique(x$cluster))
num_beta <- num_total-1-num_cluster
# Store index information
index_vk <- (num_beta+1):(num_total-1)
index_beta <- 1:num_beta
index_sigma <- num_total
index_cluster <- x$cluster
# Format data
x <- cbind(1,x) # append 1's to x for intercept term
x <- x %>% select(-cluster)
y <- as.matrix(y)
x <- as.matrix(x)
# Coefficient estimate storage - betas, vk, sigma_v
theta_hat <- matrix(NA, num_iter, num_total)
theta_hat[1,] <- theta_init
# Run algorithm
for(i in 2:num_iter){
if (verbose==T && (i%%100 == 0)){
print(i)
}
# Generate a proposal theta using a normal distribution
proposal_theta_j <- theta_hat[i-1,] + rnorm(num_total,0,jump_sigma)
for(j in 1:num_total){
# Store previous and proposal theta
prev_theta <- prop_theta <- theta_hat[i,]
prev_theta[j:num_total] <- prop_theta[j:num_total] <- theta_hat[i-1,j:num_total]
prop_theta[j] <- proposal_theta_j[j]
if (j %in% index_beta){
## Update beta terms
vk <- prev_theta[index_vk]
beta_prop <- prop_theta[index_beta]
beta_prev <- prev_theta[index_beta]
# Calculate log posterior probability of proposed and previous
log_prop <- log_lik.beta(beta_prop, y, x, vk)
log_prev <- log_lik.beta(beta_prev, y, x, vk)
}else if (j %in% index_vk){
## Update Vk terms
beta <- prop_theta[index_beta]
vk_prop <- prop_theta[j]
vk_prev <- prev_theta[j]
sv <- prev_theta[index_sigma]
# Subset y and x for cluster k
curr_cluster <- which (j==index_vk)
curr_x <- x[index_cluster==curr_cluster,]
curr_y <- y[index_cluster==curr_cluster]
# Calculate log posterior probability of proposed and previous
log_prop <- log_lik.vk(beta, curr_y, curr_x, vk_prop, sv)
log_prev <- log_lik.vk(beta, curr_y, curr_x, vk_prev, sv)
}else if (j == index_sigma){
## Update sigma_v
sv_prop <- prop_theta[j]
sv_prev <- prev_theta[j]
curr_vk <- prop_theta[index_vk]
log_prop <- log_lik.sv(sv_prop, curr_vk, num_cluster, gamma_init[1], gamma_init[2])
log_prev <- log_lik.sv(sv_prev, curr_vk, num_cluster, gamma_init[1], gamma_init[2])
}
# Calculate ratio
log_ratio <- log_prop - log_prev
# Flip coin to choose acceptance
if (log(runif(1)) < log_ratio){
theta_hat[i,j] <- prop_theta[j]
} else{
theta_hat[i,j] <- theta_hat[i-1,j]
}
}
}
theta_hat[(burn_in+2):num_iter,]
}
n_iter <- 70000
burn_in <- 3000
theta.init <- c(rep(0,ncol(dat.cl)+J-1),0.5)
gamma.init <- c(1,1)
jump_sigma <- c(0.25,0.2,0.01,0.01,rep(0.2, J),0.2)
y <- as.matrix(dat.cl %>% select(y))
x <- as.matrix(dat.cl %>% select(-y))
a
run1 <- MH.c(y, x, theta.init, gamma.init, jump_sigma, n_iter, burn_in, verbose=T)
x
x$cluster
x %>% select(cluster)
x
n_iter <- 70000
burn_in <- 3000
theta.init <- c(rep(0,ncol(dat.cl)+J-1),0.5)
gamma.init <- c(1,1)
jump_sigma <- c(0.25,0.2,0.01,0.01,rep(0.2, J),0.2)
y <- as.matrix(dat %>% select(y))
x <- cbind(1,as.matrix(dat %>% select(-y)))
y
x
y <- as.matrix(dat.cl %>% select(y))
x <- cbind(1,as.matrix(dat.cl %>% select(-y)))
y
x
x
x["cluster"]
x[,"cluster"]
x[,-"cluster"]
x[,"cluster"]
x
x[,colnames(x)!="cluster"]
x[,"cluster"]
source("BLR_func.R")
run1 <- MH.c(y, x, theta.init, gamma.init, jump_sigma, n_iter, burn_in)
source("BLR_func.R")
run1 <- MH.c(y, x, theta.init, gamma.init, jump_sigma, n_iter, burn_in)
# Metropolis-Hastings algorithm
# y - outcome values
# x - covariate matrix
# theta_init - starting beta, vk, and sigma_v values for MH algorithm
# gamma_init - starting gamma prior distribution parameters
# jump_sigma - variance for proposal distribution
# n_iter - number of MH steps
# burn_in - number of iterations to remove for burn-in
MH.c <- function(y, x, theta_init, gamma_init, jump_sigma, n_iter, burn_in){
# Total # of iterations
num_iter <- n_iter + burn_in + 1
# Record count information
num_total <- length(theta_init)
num_cluster <- length(x[,"cluster"])
num_beta <- num_total-1-num_cluster
# Store index information
index_vk <- (num_beta+1):(num_total-1)
index_beta <- 1:num_beta
index_sigma <- num_total
index_cluster <- x[,"cluster"]
# Format data
x <- x[,colnames(x)!="cluster"]
# Coefficient estimate storage - betas, vk, sigma_v
theta_hat <- matrix(NA, num_iter, num_total)
theta_hat[1,] <- theta_init
browser()
# Run algorithm
for(i in 2:num_iter){
# Generate a proposal theta using a normal distribution
proposal_theta_j <- theta_hat[i-1,] + rnorm(num_total,0,jump_sigma)
for(j in 1:num_total){
# Store previous and proposal theta
prev_theta <- prop_theta <- theta_hat[i,]
prev_theta[j:num_total] <- prop_theta[j:num_total] <- theta_hat[i-1,j:num_total]
prop_theta[j] <- proposal_theta_j[j]
if (j %in% index_beta){
## Update beta terms
vk <- prev_theta[index_vk]
beta_prop <- prop_theta[index_beta]
beta_prev <- prev_theta[index_beta]
# Calculate log posterior probability of proposed and previous
log_prop <- log_lik.beta(beta_prop, y, x, vk)
log_prev <- log_lik.beta(beta_prev, y, x, vk)
}else if (j %in% index_vk){
## Update Vk terms
beta <- prop_theta[index_beta]
vk_prop <- prop_theta[j]
vk_prev <- prev_theta[j]
sv <- prev_theta[index_sigma]
# Subset y and x for cluster k
curr_cluster <- which (j==index_vk)
curr_x <- x[index_cluster==curr_cluster,]
curr_y <- y[index_cluster==curr_cluster]
# Calculate log posterior probability of proposed and previous
log_prop <- log_lik.vk(beta, curr_y, curr_x, vk_prop, sv)
log_prev <- log_lik.vk(beta, curr_y, curr_x, vk_prev, sv)
}else if (j == index_sigma){
## Update sigma_v
sv_prop <- prop_theta[j]
sv_prev <- prev_theta[j]
curr_vk <- prop_theta[index_vk]
log_prop <- log_lik.sv(sv_prop, curr_vk, num_cluster, gamma_init[1], gamma_init[2])
log_prev <- log_lik.sv(sv_prev, curr_vk, num_cluster, gamma_init[1], gamma_init[2])
}
# Calculate ratio
log_ratio <- log_prop - log_prev
# Flip coin to choose acceptance
if (log(runif(1)) < log_ratio){
theta_hat[i,j] <- prop_theta[j]
} else{
theta_hat[i,j] <- theta_hat[i-1,j]
}
}
}
theta_hat[(burn_in+2):num_iter,]
}
run1 <- MH.c(y, x, theta.init, gamma.init, jump_sigma, n_iter, burn_in)
num_iter
num_total
theta_init
num_cluster
num_beta
# Metropolis-Hastings algorithm
# y - outcome values
# x - covariate matrix
# theta_init - starting beta, vk, and sigma_v values for MH algorithm
# gamma_init - starting gamma prior distribution parameters
# jump_sigma - variance for proposal distribution
# n_iter - number of MH steps
# burn_in - number of iterations to remove for burn-in
MH.c <- function(y, x, theta_init, gamma_init, jump_sigma, n_iter, burn_in){
# Total # of iterations
num_iter <- n_iter + burn_in + 1
# Record count information
num_total <- length(theta_init)
num_cluster <- length(unique(x[,"cluster"]))
num_beta <- num_total-1-num_cluster
# Store index information
index_vk <- (num_beta+1):(num_total-1)
index_beta <- 1:num_beta
index_sigma <- num_total
index_cluster <- x[,"cluster"]
# Format data
x <- x[,colnames(x)!="cluster"]
# Coefficient estimate storage - betas, vk, sigma_v
theta_hat <- matrix(NA, num_iter, num_total)
theta_hat[1,] <- theta_init
browser()
# Run algorithm
for(i in 2:num_iter){
# Generate a proposal theta using a normal distribution
proposal_theta_j <- theta_hat[i-1,] + rnorm(num_total,0,jump_sigma)
for(j in 1:num_total){
# Store previous and proposal theta
prev_theta <- prop_theta <- theta_hat[i,]
prev_theta[j:num_total] <- prop_theta[j:num_total] <- theta_hat[i-1,j:num_total]
prop_theta[j] <- proposal_theta_j[j]
if (j %in% index_beta){
## Update beta terms
vk <- prev_theta[index_vk]
beta_prop <- prop_theta[index_beta]
beta_prev <- prev_theta[index_beta]
# Calculate log posterior probability of proposed and previous
log_prop <- log_lik.beta(beta_prop, y, x, vk)
log_prev <- log_lik.beta(beta_prev, y, x, vk)
}else if (j %in% index_vk){
## Update Vk terms
beta <- prop_theta[index_beta]
vk_prop <- prop_theta[j]
vk_prev <- prev_theta[j]
sv <- prev_theta[index_sigma]
# Subset y and x for cluster k
curr_cluster <- which (j==index_vk)
curr_x <- x[index_cluster==curr_cluster,]
curr_y <- y[index_cluster==curr_cluster]
# Calculate log posterior probability of proposed and previous
log_prop <- log_lik.vk(beta, curr_y, curr_x, vk_prop, sv)
log_prev <- log_lik.vk(beta, curr_y, curr_x, vk_prev, sv)
}else if (j == index_sigma){
## Update sigma_v
sv_prop <- prop_theta[j]
sv_prev <- prev_theta[j]
curr_vk <- prop_theta[index_vk]
log_prop <- log_lik.sv(sv_prop, curr_vk, num_cluster, gamma_init[1], gamma_init[2])
log_prev <- log_lik.sv(sv_prev, curr_vk, num_cluster, gamma_init[1], gamma_init[2])
}
# Calculate ratio
log_ratio <- log_prop - log_prev
# Flip coin to choose acceptance
if (log(runif(1)) < log_ratio){
theta_hat[i,j] <- prop_theta[j]
} else{
theta_hat[i,j] <- theta_hat[i-1,j]
}
}
}
theta_hat[(burn_in+2):num_iter,]
}
run1 <- MH.c(y, x, theta.init, gamma.init, jump_sigma, n_iter, burn_in)
num_cluster
num_beta
index_vk
index_beta
index_sigma
index_cluster
x
head(theta_hat)
# Metropolis-Hastings algorithm
# y - outcome values
# x - covariate matrix
# theta_init - starting beta, vk, and sigma_v values for MH algorithm
# gamma_init - starting gamma prior distribution parameters
# jump_sigma - variance for proposal distribution
# n_iter - number of MH steps
# burn_in - number of iterations to remove for burn-in
MH.c <- function(y, x, theta_init, gamma_init, jump_sigma, n_iter, burn_in){
# Total # of iterations
num_iter <- n_iter + burn_in + 1
# Record count information
num_total <- length(theta_init)
num_cluster <- length(unique(x[,"cluster"]))
num_beta <- num_total-1-num_cluster
# Store index information
index_vk <- (num_beta+1):(num_total-1)
index_beta <- 1:num_beta
index_sigma <- num_total
index_cluster <- x[,"cluster"]
# Format data
x <- x[,colnames(x)!="cluster"]
# Coefficient estimate storage - betas, vk, sigma_v
theta_hat <- matrix(NA, num_iter, num_total)
theta_hat[1,] <- theta_init
# Run algorithm
for(i in 2:num_iter){
# Generate a proposal theta using a normal distribution
proposal_theta_j <- theta_hat[i-1,] + rnorm(num_total,0,jump_sigma)
for(j in 1:num_total){
# Store previous and proposal theta
prev_theta <- prop_theta <- theta_hat[i,]
prev_theta[j:num_total] <- prop_theta[j:num_total] <- theta_hat[i-1,j:num_total]
prop_theta[j] <- proposal_theta_j[j]
if (j %in% index_beta){
## Update beta terms
vk <- prev_theta[index_vk]
beta_prop <- prop_theta[index_beta]
beta_prev <- prev_theta[index_beta]
# Calculate log posterior probability of proposed and previous
log_prop <- log_lik.beta(beta_prop, y, x, vk)
log_prev <- log_lik.beta(beta_prev, y, x, vk)
}else if (j %in% index_vk){
## Update Vk terms
beta <- prop_theta[index_beta]
vk_prop <- prop_theta[j]
vk_prev <- prev_theta[j]
sv <- prev_theta[index_sigma]
# Subset y and x for cluster k
curr_cluster <- which (j==index_vk)
curr_x <- x[index_cluster==curr_cluster,]
curr_y <- y[index_cluster==curr_cluster]
# Calculate log posterior probability of proposed and previous
log_prop <- log_lik.vk(beta, curr_y, curr_x, vk_prop, sv)
log_prev <- log_lik.vk(beta, curr_y, curr_x, vk_prev, sv)
}else if (j == index_sigma){
## Update sigma_v
sv_prop <- prop_theta[j]
sv_prev <- prev_theta[j]
curr_vk <- prop_theta[index_vk]
log_prop <- log_lik.sv(sv_prop, curr_vk, num_cluster, gamma_init[1], gamma_init[2])
log_prev <- log_lik.sv(sv_prev, curr_vk, num_cluster, gamma_init[1], gamma_init[2])
}
# Calculate ratio
log_ratio <- log_prop - log_prev
# Flip coin to choose acceptance
if (log(runif(1)) < log_ratio){
theta_hat[i,j] <- prop_theta[j]
} else{
theta_hat[i,j] <- theta_hat[i-1,j]
}
}
}
theta_hat[(burn_in+2):num_iter,]
}
run1 <- MH.c(y, x, theta.init, gamma.init, jump_sigma, n_iter, burn_in)
beta
vk
dim(vk)
beta
dim(y)
length(y)
dim(x)
sigma_vk
sv
sigma_v
vk
theta
beta
source("BLR_func.R")
source("BLR_func.R")
test()
dnorm(2)
source("BLR_func.R")
test()
# Conditional log likelihood for sigmav
log_lik.sv <- function(sigma_v, vk, num_cluster, a, b){
browser()
dgamma(sigma_v, shape=a + num_cluster/2, rate=b + sum(vk^2)/2, log = T)
}
run1 <- MH.c(y, x, theta.init, gamma.init, jump_sigma, n_iter, burn_in)
sigma_v
vk
num_cluster
a
b
source("BLR_func.R")
test()
3.5+2.5+1
source("BLR_func.R")
test()
source("BLR_func.R")
sample_int()
sample_int()
sample_int()
samp()
samp()
samp()
sample_int()
source("BLR_func.R")
test(c(0,1,2,3),15,c(0.4,0.1,0.1,0.4))
samp(c(0,1,2,3),15,c(0.4,0.1,0.1,0.4))
samp(c(0,1,2,3),15,c(0.4,0.1,0.1,0.4))
samp(c(0,1,2,3),15,c(0.4,0.1,0.1,0.4))
samp(c(0,1,2,3),150,c(0.4,0.1,0.1,0.4))
library(dplyr)
library(lme4)
source("BLR_func.R")
source("Helper Files/BLR_Data_Gen.R")
set.seed(4)
dat <- gen_data(param=c(14.24,0.22,-0.07,-0.44), x=design_matrix, n_j=n, J=1, sigma_v=1)
dat <- dat %>% select(-cluster, -V_j)
n_iter <- 10000
burn_in <- 1000
beta.init <- rep(0,ncol(dat))
jump_sigma <- c(0.25,0.2,0.01,0.01)
y <- as.matrix(dat %>% select(y))
x <- cbind(1,as.matrix(dat %>% select(-y)))
# Run algorithm
run1 <- MH.u(y, x, beta.init, jump_sigma, n_iter, burn_in)
tail(run1)
head(run1)
source("BLR_func.R")
# Run algorithm
run1 <- MH.u(y, x, beta.init, jump_sigma, n_iter, burn_in)
tail(run)
tail(run1)
source("BLR_func.R")
# Run algorithm
run1 <- MH.u(y, x, beta.init, jump_sigma, n_iter, burn_in)
tail(run1)
source("BLR_func.R")
# Run algorithm
run1 <- MHu(y, x, beta.init, jump_sigma, n_iter, burn_in)
source("BLR_func.R")
# Run algorithm
run1 <- MHu(y, x, beta.init, jump_sigma, n_iter, burn_in)
tail(run1)
### Bayesian Linear Regression - Main Script ####
library(dplyr)
library(lme4)
library(Rcpp)
require(RcppArmadillo)
sourceCpp("BLR_func.cpp")
source("Helper Files/BLR_Data_Gen.R")
## Unclustered simulation
set.seed(4)
dat <- gen_data(param=c(14.24,0.22,-0.07,-0.44), x=design_matrix, n_j=n, J=1, sigma_v=1)
dat <- dat %>% select(-cluster, -V_j)
n_iter <- 100000
burn_in <- 10000
beta.init <- rep(0,ncol(dat))
jump_sigma <- c(0.25,0.2,0.01,0.01)
y <- as.matrix(dat %>% select(y))
x <- cbind(1,as.matrix(dat %>% select(-y)))
# Run algorithm
run1 <- MHu(y, x, beta.init, jump_sigma, n_iter, burn_in)
# Run algorithm
ptm <- proc.time()
run1 <- MHu(y, x, beta.init, jump_sigma, n_iter, burn_in)
ptm - proc.time()
